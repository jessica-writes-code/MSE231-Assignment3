table(test_data$V1, test_bin_preds)
95+112/(sum(table(test_data$V1, test_bin_preds)))
(95+112)/(sum(table(test_data$V1, test_bin_preds)))
install.packages("ROCR")
library(ROCR)
?prediction
pred <- prediction(test_bin_preds, test_data$V1)
pred <- prediction(test_predictions, test_data$V1)
test_labels <- ifelse(test_data$V1=="Trump",1,-1)
pred <- prediction(test_predictions, test_labels)
perf <- performance(pred,"tpr","fpr")
plot(perf)
ggplot(perf)
library(ggplot)
library(ggplot2)
ggplot(perf)
performance(perf, "auc")
performance(pred, "auc")
performance(pred, "auc")[[1]]
performance(pred, "auc")[1]
performance(pred, "auc")['y.values']
performance(pred, "auc")$y.values
performance(pred, "auc").y.values
# Remember to change this to not reference the data folder
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE)
labels <- ifelse(test_data$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(test_predictions, test_labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".png"))
plot(perf)
dev.off()
return(performance(pred, "auc"))
}
make_roc("./data/test_data.tsv","./data/test_predictions.txt","./test_plot")
setwd("~/Dropbox/Graduate_School/MSE231/MSE231-Assignment3")
make_roc("./data/test_data.tsv","./data/test_predictions.txt","./test_plot")
make_roc("./data/test_data.txt","./data/test_predictions.txt","./test_plot")
make_roc("./data/train_data.txt","./data/train_predictions.txt","./test_plot")
make_roc("./data/test_data.txt","./data/test_predictions.txt","./test_roc")
make_roc("./data/train_data.txt","./data/train_predictions.txt","./train_roc")
make_roc("./data/train_data.txt","./data/training_predictions.txt","./train_roc")
make_roc("./data/train_data.txt","./data/training_predictions.txt","./train_roc")
# Remember to change this to not reference the data folder
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE)
labels <- ifelse(test_data$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(test_predictions, test_labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".png"))
plot(perf)
dev.off()
return(performance(pred, "auc"))
}
make_roc("./data/test_data.txt","./data/test_predictions.txt","./test_roc")
make_roc("./data/training_data.txt","./data/training_predictions.txt","./train_roc")
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE)
labels <- ifelse(test_data$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(test_predictions, test_labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".png"))
plot(perf)
dev.off()
return(performance(pred, "auc"))
}
make_roc("./data/test_data.txt","./data/test_predictions.txt","./test_roc")
# Remember to change this to not reference the data folder
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE)
labels <- ifelse(data$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(test_predictions, test_labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".png"))
plot(perf)
dev.off()
return(performance(pred, "auc"))
}
make_roc("./data/test_data.txt","./data/test_predictions.txt","./test_roc")
# Remember to change this to not reference the data folder
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE)
labels <- ifelse(df$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(test_predictions, test_labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".png"))
plot(perf)
dev.off()
return(performance(pred, "auc"))
}
make_roc("./data/test_data.txt","./data/test_predictions.txt","./test_roc")
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE)
labels <- ifelse(df$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(vw_preds, test_labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".png"))
plot(perf)
dev.off()
return(performance(pred, "auc"))
}
make_roc("./data/test_data.txt","./data/test_predictions.txt","./test_roc")
# Remember to change this to not reference the data folder
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE)
labels <- ifelse(df$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(vw_preds, labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".png"))
plot(perf)
dev.off()
return(performance(pred, "auc"))
}
make_roc("./data/test_data.txt","./data/test_predictions.txt","./test_roc")
df <- read.csv("./data/test_data.txt", sep = "\t", header=FALSE)
vw_preds <- read.table(./data/test_predictions.txt)
vw_preds <- read.table("./data/test_predictions.txt")
View(df)
View(vw_preds)
make_roc("./data/test_tweets.tsv","./data/test_predictions.txt","./test_roc")
make_roc("./data/train_tweets.txt","./data/training_predictions.txt","./train_roc")
make_roc("./data/train_tweets.tsv","./data/training_predictions.txt","./train_roc")
df <- read.csv("./data/train_tweets.tsv", sep = "\t", header=FALSE)
?read.csv
# Remember to change this to not reference the data folder
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(vw_preds, labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".png"))
plot(perf)
dev.off()
return(performance(pred, "auc"))
}
make_roc("./data/train_tweets.tsv","./data/training_predictions.txt","./train_roc")
data <- "./data/test_tweets.tsv"
predictions <- "./data/test_predictions.txt"
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
summary(vw_preds)
pred_bins <- round(vw_preds,1)
View(pred_bins)
pred_bins <- round(vw_preds)
pred_df <- data.frame(cbind(labels,pred_bins))
View(pred_df)
names(pred_df) <- c("labels","prediction")
View(pred_df)
pred_df <- group_by(pred_df, prediction) %>%
summarize(avg_trump=mean(labels))
library(dplyr)
pred_df <- group_by(pred_df, prediction) %>%
summarize(avg_trump=mean(labels))
pred_df
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,0)
vw_preds <- read.table(predictions)
pred_bins <- round(vw_preds)
pred_df <- data.frame(cbind(labels,pred_bins))
names(pred_df) <- c("labels","prediction")
pred_df_sum <- group_by(pred_df, prediction) %>%
summarize(avg_trump=mean(labels), n = n())
pred_df
pred_df_sum
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point()
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point() +
labs(x="Model Prediction","Empirical Rate")
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,0)
vw_preds <- read.table(predictions)
pred_bins <- 1/(1+e^vw_preds)
vw_preds <- read.table(predictions)
pred_bins <- 1/(1+exp(vw_preds))
pred_df <- data.frame(cbind(labels,pred_bins))
names(pred_df) <- c("labels","prediction")
pred_df_sum <- group_by(pred_df, prediction) %>%
summarize(avg_trump=mean(labels), n = n())
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point() +
labs(x="Model Prediction","Empirical Rate") +
lksdjf
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,0)
vw_preds <- read.table(predictions)
pred_bins <- round(1/(1+exp(vw_preds)),1)
pred_df <- data.frame(cbind(labels,pred_bins))
names(pred_df) <- c("labels","prediction")
pred_df_sum <- group_by(pred_df, prediction) %>%
summarize(avg_trump=mean(labels), n = n())
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point() +
labs(x="Model Prediction","Empirical Rate")
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,0)
vw_preds <- read.table(predictions)
pred_bins <- round(1/(1+exp(-vw_preds)),1)
pred_df <- data.frame(cbind(labels,pred_bins))
names(pred_df) <- c("labels","prediction")
pred_df_sum <- group_by(pred_df, prediction) %>%
summarize(avg_trump=mean(labels), n = n())
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point() +
labs(x="Model Prediction","Empirical Rate")
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point() +
labs(x="Model Prediction","Empirical Rate") +
theme(legend.position="none")
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point() +
labs(x="Model Prediction","Empirical Rate") +
theme(legend.position="none") +
scale_x_continuous(expand=c(0,0))
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point() +
labs(x="Model Prediction","Empirical Rate") +
theme(legend.position="none")
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point() +
labs(x="Model Prediction",y="Empirical Rate") +
theme(legend.position="none")
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point() +
labs(x="Model Prediction",y="Empirical Rate") +
theme(legend.position="none") +
scale_x_continuous(labels = scales::percent) +
scale_y_continuous(labels = scales::percent)
theme_set(theme_bw())
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point() +
labs(x="Model Prediction",y="Empirical Rate") +
theme(legend.position="none") +
scale_x_continuous(labels = scales::percent) +
scale_y_continuous(labels = scales::percent)
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point(alpha=0.5) +
labs(x="Model Prediction",y="Empirical Rate") +
theme(legend.position="none") +
scale_x_continuous(labels = scales::percent) +
scale_y_continuous(labels = scales::percent)
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point(alpha=0.7) +
labs(x="Model Prediction",y="Empirical Rate") +
theme(legend.position="none") +
scale_x_continuous(labels = scales::percent) +
scale_y_continuous(labels = scales::percent) +
geom_abline(slope=1, intercept=0)
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point(alpha=0.7) +
labs(x="Model Prediction",y="Empirical Rate") +
theme(legend.position="none") +
scale_x_continuous(labels = scales::percent) +
scale_y_continuous(labels = scales::percent) +
geom_abline(slope=1, intercept=0, linetype=2)
library(dplyr)
library(ggplot2)
library(ROCR)
library(scales)
## Function to make ROC curve & AUC
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(vw_preds, labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".pdf"))
plot(perf)
dev.off()
return(performance(pred, "auc"))
}
## Function to make calibration plots
make_calib <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,0)
vw_preds <- read.table(predictions)
pred_bins <- round(1/(1+exp(-vw_preds)),1)
pred_df <- data.frame(cbind(labels,pred_bins))
names(pred_df) <- c("labels","prediction")
pred_df_sum <- group_by(pred_df, prediction) %>%
summarize(avg_trump=mean(labels), n = n())
theme_set(theme_bw())
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point(alpha=0.7) +
labs(x="Model Prediction",y="Empirical Rate") +
theme(legend.position="none") +
scale_x_continuous(labels = scales::percent) +
scale_y_continuous(labels = scales::percent) +
geom_abline(slope=1, intercept=0, linetype=2)
ggsave(paste0(plot_loc,".pdf"))
}
make_roc("./data/test_tweets.tsv","./data/test_predictions.txt","./test_roc")
make_roc("./data/train_tweets.tsv","./data/training_predictions.txt","./train_roc")
make_calib("./data/test_tweets.tsv","./data/test_predictions.txt","./test_calib")
make_calib("./data/train_tweets.tsv","./data/training_predictions.txt","./train_calib")
library(dplyr)
library(ggplot2)
library(ROCR)
library(scales)
## Function to make ROC curve & AUC
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(vw_preds, labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".pdf"))
plot(perf)
dev.off()
return(performance(pred, "auc")@y.values)
}
## Function to make calibration plots
make_calib <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,0)
vw_preds <- read.table(predictions)
pred_bins <- round(1/(1+exp(-vw_preds)),1)
pred_df <- data.frame(cbind(labels,pred_bins))
names(pred_df) <- c("labels","prediction")
pred_df_sum <- group_by(pred_df, prediction) %>%
summarize(avg_trump=mean(labels), n = n())
theme_set(theme_bw())
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point(alpha=0.7) +
labs(x="Model Prediction",y="Empirical Rate") +
theme(legend.position="none") +
scale_x_continuous(labels = scales::percent) +
scale_y_continuous(labels = scales::percent) +
geom_abline(slope=1, intercept=0, linetype=2)
ggsave(paste0(plot_loc,".pdf"))
}
## Run Plots
# Remember to change this to not reference the data folder
make_roc("./data/test_tweets.tsv","./data/test_predictions.txt","./test_roc")
make_roc("./data/train_tweets.tsv","./data/training_predictions.txt","./train_roc")
make_calib("./data/test_tweets.tsv","./data/test_predictions.txt","./test_calib")
make_calib("./data/train_tweets.tsv","./data/training_predictions.txt","./train_calib")
library(dplyr)
library(ggplot2)
library(ROCR)
library(scales)
## Function to make ROC curve & AUC
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(vw_preds, labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".pdf"))
plot(perf)
dev.off()
return(performance(pred, "auc")@y.values[[1]])
}
## Function to make calibration plots
make_calib <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,0)
vw_preds <- read.table(predictions)
pred_bins <- round(1/(1+exp(-vw_preds)),1)
pred_df <- data.frame(cbind(labels,pred_bins))
names(pred_df) <- c("labels","prediction")
pred_df_sum <- group_by(pred_df, prediction) %>%
summarize(avg_trump=mean(labels), n = n())
theme_set(theme_bw())
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point(alpha=0.7) +
labs(x="Model Prediction",y="Empirical Rate") +
theme(legend.position="none") +
scale_x_continuous(labels = scales::percent) +
scale_y_continuous(labels = scales::percent) +
geom_abline(slope=1, intercept=0, linetype=2)
ggsave(paste0(plot_loc,".pdf"))
}
## Run Plots
# Remember to change this to not reference the data folder
make_roc("./data/test_tweets.tsv","./data/test_predictions.txt","./test_roc")
make_roc("./data/train_tweets.tsv","./data/training_predictions.txt","./train_roc")
make_calib("./data/test_tweets.tsv","./data/test_predictions.txt","./test_calib")
make_calib("./data/train_tweets.tsv","./data/training_predictions.txt","./train_calib")
library(dplyr)
library(ggplot2)
library(ROCR)
library(scales)
## Function to make ROC curve & AUC
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(vw_preds, labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".pdf"))
plot(perf)
dev.off()
return(performance(pred, "auc")@y.values[[1]])
}
## Function to make calibration plots
make_calib <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,0)
vw_preds <- read.table(predictions)
pred_bins <- round(1/(1+exp(-vw_preds)),1)
pred_df <- data.frame(cbind(labels,pred_bins))
names(pred_df) <- c("labels","prediction")
pred_df_sum <- group_by(pred_df, prediction) %>%
summarize(avg_trump=mean(labels), n = n())
theme_set(theme_bw())
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point(alpha=0.7) +
labs(x="Model Prediction",y="Empirical Rate") +
theme(legend.position="none") +
scale_x_continuous(labels = scales::percent) +
scale_y_continuous(labels = scales::percent) +
geom_abline(slope=1, intercept=0, linetype=2)
ggsave(paste0(plot_loc,".pdf"))
}
## Run Plots
make_roc("./test_tweets.tsv","./test_predictions.txt","./test_roc")
make_roc("./train_tweets.tsv","./training_predictions.txt","./train_roc")
make_calib("./test_tweets.tsv","./test_predictions.txt","./test_calib")
make_calib("./train_tweets.tsv","./training_predictions.txt","./train_calib")
setwd("~/Dropbox/Graduate_School/MSE231/MSE231-Assignment3")
library(dplyr)
library(ggplot2)
library(ROCR)
library(scales)
## Function to make ROC curve & AUC
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(vw_preds, labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".pdf"))
plot(perf)
dev.off()
return(performance(pred, "auc")@y.values[[1]])
}
## Function to make calibration plots
make_calib <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,0)
vw_preds <- read.table(predictions)
pred_bins <- round(1/(1+exp(-vw_preds)),1)
pred_df <- data.frame(cbind(labels,pred_bins))
names(pred_df) <- c("labels","prediction")
pred_df_sum <- group_by(pred_df, prediction) %>%
summarize(avg_trump=mean(labels), n = n())
theme_set(theme_bw())
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point(alpha=0.7) +
labs(x="Model Prediction",y="Empirical Rate") +
theme(legend.position="none") +
scale_x_continuous(labels = scales::percent) +
scale_y_continuous(labels = scales::percent) +
geom_abline(slope=1, intercept=0, linetype=2)
ggsave(paste0(plot_loc,".pdf"))
}
## Run Plots
make_roc("./test_tweets.tsv","./test_predictions.txt","./test_roc")
make_roc("./train_tweets.tsv","./training_predictions.txt","./train_roc")
make_calib("./test_tweets.tsv","./test_predictions.txt","./test_calib")
make_calib("./train_tweets.tsv","./training_predictions.txt","./train_calib")
library(dplyr)
library(ggplot2)
library(ROCR)
library(scales)
## Function to make ROC curve & AUC
make_roc <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,-1)
vw_preds <- read.table(predictions)
pred <- prediction(vw_preds, labels)
perf <- performance(pred,"tpr","fpr")
png(filename=paste0(plot_loc,".pdf"))
plot(perf)
dev.off()
return(performance(pred, "auc")@y.values[[1]])
}
## Function to make calibration plots
make_calib <- function(data, predictions,plot_loc) {
df <- read.csv(data, sep = "\t", header=FALSE, quote="")
labels <- ifelse(df$V1=="Trump",1,0)
vw_preds <- read.table(predictions)
pred_bins <- round(1/(1+exp(-vw_preds)),1)
pred_df <- data.frame(cbind(labels,pred_bins))
names(pred_df) <- c("labels","prediction")
pred_df_sum <- group_by(pred_df, prediction) %>%
summarize(avg_trump=mean(labels), n = n())
theme_set(theme_bw())
ggplot(pred_df_sum, aes(x=prediction, y=avg_trump, size=n)) +
geom_point(alpha=0.7) +
labs(x="Model Prediction",y="Empirical Rate") +
theme(legend.position="none") +
scale_x_continuous(labels = scales::percent) +
scale_y_continuous(labels = scales::percent) +
geom_abline(slope=1, intercept=0, linetype=2)
ggsave(paste0(plot_loc,".pdf"))
}
## Run Plots
make_roc("./test_tweets.tsv","./test_predictions.txt","./test_roc")
make_roc("./train_tweets.tsv","./training_predictions.txt","./train_roc")
make_calib("./test_tweets.tsv","./test_predictions.txt","./test_calib")
make_calib("./train_tweets.tsv","./training_predictions.txt","./train_calib")
